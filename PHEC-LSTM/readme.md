# [Homework#03] Dá»± ÄoÃ¡n Má»©c TiÃªu Thá»¥ Äiá»‡n NÄƒng dÃ¹ng LSTM
# Há»c ViÃªn: HoÃ ng HÃ o
> **HÆ°á»›ng** **Dáº«n:** **Dá»±** **ÄoÃ¡n** **Má»©c** **TiÃªu** **Thá»¥** **Äiá»‡n**
> **NÄƒng** **Há»™** **Gia** **ÄÃ¬nh** **Báº±ng** **LSTM**

**1.** **Má»¥c** **TiÃªu**

> Sá»­ dá»¥ng dá»¯ liá»‡u Ä‘iá»‡n nÄƒng cá»§a má»™t há»™ gia Ä‘Ã¬nh Ä‘á»ƒ dá»± Ä‘oÃ¡n má»©c tiÃªu thá»¥
> Ä‘iá»‡n nÄƒng trong tÆ°Æ¡ng lai gáº§n.
>
> á»¨ng dá»¥ng mÃ´ hÃ¬nh LSTM cho bÃ i toÃ¡n dá»± bÃ¡o chuá»—i thá»i gian.
>
> So sÃ¡nh hiá»‡u nÄƒng khi thay Ä‘á»•i cÃ¡c hyperparameter nhÆ° window_size,
> units, batch_size,...

**2.** **Kiáº¿n** **Thá»©c** **Cáº§n** **CÃ³**

**A.** **Vá»** **Dá»¯** **Liá»‡u** **Thá»i** **Gian**

> Hiá»ƒu vá» chuá»—i thá»i gian (time series).
>
> KhÃ¡i niá»‡m trá»… thá»i gian (lag), sliding window.

**B.** **Vá»** **Deep** **Learning**

> Kiáº¿n thá»©c cÆ¡ báº£n vá» Máº¡ng NÆ¡-ron NhÃ¢n Táº¡o (NN).
>
> Cáº¥u trÃºc cá»§a RNN vÃ  lÃ½ do dÃ¹ng LSTM Ä‘á»ƒ xá»­ lÃ½ chuá»—i dÃ i.
>
> Hiá»ƒu cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a cÃ¡c gate trong LSTM: forget, input, output
> gate.

**C.** **Vá»** **Ká»¹** **Thuáº­t**

> Biáº¿t sá»­ dá»¥ng thÆ° viá»‡n Python: pandas, numpy, matplotlib, scikit-learn,
> tensorflow.keras.
>
> Biáº¿t cÃ¡ch chuáº©n hÃ³a dá»¯ liá»‡u, chia train/test, Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh báº±ng
> MAE.

**3.** **Dá»¯** **Liá»‡u**

Nguá»“n:

[<u>https://archive.ics.uci.edu/static/public/235/individual+househ</u>](https://archive.ics.uci.edu/static/public/235/individual+household+electric+power+consumption.zip)
[<u>old+electric+power+consumption.zip</u>](https://archive.ics.uci.edu/static/public/235/individual+household+electric+power+consumption.zip)

**Cá»™t** **sá»­** **dá»¥ng:**

\['Global_active_power', 'Global_reactive_power', 'Voltage',
'Global_intensity', 'Sub_metering_1', 'Sub_metering_2',
'Sub_metering_3'\]

**4.** **XÃ¢y** **Dá»±ng** **MÃ´** **HÃ¬nh** **LSTM**

Táº¡o mÃ´ hÃ¬nh tuáº§n tá»± gá»“m 2 lá»›p:

> **LSTM**: dÃ¹ng khoáº£ng 50 Ä‘Æ¡n vá»‹, nháº­n Ä‘áº§u vÃ o cÃ³ window_size bÆ°á»›c vÃ  7
> Ä‘áº·c trÆ°ng.
>
> **Dense**: 1 node Ä‘á»ƒ dá»± Ä‘oÃ¡n Global_active_power.

BiÃªn dá»‹ch mÃ´ hÃ¬nh vá»›i:

> Tá»‘i Æ°u: adam
>
> HÃ m máº¥t mÃ¡t: mse

YÃªu cáº§u: Thay Ä‘á»•i cÃ¡c tham sá»‘ (epochs, window_size, batch_size, vÃ 
units), hÃ m tá»‘i Æ°u, hÃ m máº¥t mÃ¡t vÃ  rÃºt ra nháº­n xÃ©t sá»± áº£nh hÆ°á»Ÿng Ä‘áº¿n Ä‘á»™
chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh

**5.** **TÃ i** **liá»‡u** **tham** **kháº£o:**

**<u>LSTM</u>**

<img src="./static/hcpgg0to.png"
style="width:0.19444in;height:0.19444in" />ğŸ‘‰
[<u>https://colah.github.io/posts/2015-08-Understanding-LSTMs/</u>](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

**<u>CS231n - Stanford (chÆ°Æ¡ng 1 vÃ  2):</u>**

<img src="./static/0thdjm2w.png"
style="width:0.19444in;height:0.19444in" />ğŸ‘‰
[<u>https://cs231n.github.io/neural-networks-1/</u>](https://cs231n.github.io/neural-networks-1/)

<img src="./static/rrqbniqd.png"
style="width:0.19444in;height:0.19444in" />ğŸ‘‰
[<u>https://cs231n.github.io/neural-networks-2/</u>](https://cs231n.github.io/neural-networks-2/)

(Giáº£i thÃ­ch chi tiáº¿t vá» forward pass, loss function, backpropagation)
